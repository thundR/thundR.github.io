---
layout: post
title:  "NCL Part 1 - Log Parsing"
date:   2018-04-11 14:00:00
categories: 
tags: featured
image: /assets/images/logpic.png
---

NCL is an online CTF competition that I've participated in a couple times. This time around, I thought documenting some of my findings would be helpful, since I find myself looking up the same material over and over. This is going to be pretty scattershot in terms of subjects. 

## Log Parsing 
---
Most of the time, there is a section asking for some grep/sed/awk gymnastics, and I always end up with long spaghetti strings of pipes and redirects.

#### Task 1: fail2ban.log
---

We are given a long and somewhat frightening [fail2ban log]({{ "/assets/fail2ban.log" | absolute_url }})  of critical failures and errors, and asked to parse it for information. I'll only be covering the non-trivial questions asked. 

##### 1. Find the number of unique IPs banned. 
---
Fail2ban shows a Ban event in the log with a string like this:
`2016-11-28 01:10:19,438 fail2ban.actions: WARNING [ssh] Ban 195.223.55.28`

For this task, we need to be able to do a couple of things. First, we need to be able to take output and remove duplicates, since the log contains many duplicate Ban events. We also need to be able to remove output before the matching string, since including this output would make unique sorting impossible, as the date changes with every Ban. I'll give the command string I used and break it up into parts:

`cat fail2ban.log | grep -E -o "Ban.{0,20}" | sort -u | wc`

`grep -E -o "Ban.{0,20}` allows for the `Ban` string to be matched, and only shows output 20 characters to the right. This allows for the IP address to be part of the output, while removing the date string. `sort -u` takes only the unique strings passed to it, and `wc` counts up the total lines. This gave us an answer of `8` unique IPs banned. 

##### 2. Finding which IP was banned the most.
---

We don't really need to go crazy on this one, since there are only a handful of banned IPs, and counting them manually is easy. If we wanted to scale this up, maybe with a log file of hundreds of thousands of entries, we would need to bring in a new set of commands to our string.

To start, we need to be able to pipe the output of our nicely sorted list of IPs back into grep to find occurances. This could be done easily with a bash script, but I wanted to see how far I could push a oneliner. To start, we need `xargs -i` for command output redirection. `-i` lets us insert the output of our last command into a new command as an argument variable, like so:

`cat fail2ban.log | grep -E -o "Ban.{0,20}" | sort -u | xargs -iarg grep -c arg fail2ban.log`

This gives us a nicely formatted list of the banned ips, but without their corresponding address strings:

{% highlight ruby %}
1
14
1
1
1
1
1
4
{% endhighlight %}

While this is the information we need, in a large file this can be unhelpful, since we need the corresponding address next to the number of times banned to be able to relate the data. I tried to push a oneliner to do this, but it became apparent that if I were to do this in one line, it would be a mess of formatting and garbage. The cleanest I could barely call one line came out to something like this:

`var=$(cat fail2ban.log | grep -E -o "Ban.{0,20}" | sort -u) && echo $var | xargs -iarg grep -c arg fail2ban.log && echo $var`

This gives us:

{% highlight ruby %}
1
14
1
1
1
1
1
4
Ban 108.58.9.206
Ban 116.31.116.47
Ban 12.70.197.135
Ban 195.223.55.28
Ban 47.202.16.90
Ban 91.224.160.106
Ban 91.224.160.108
Ban 92.252.94.69
{% endhighlight %}

Which is close enough. To do this properly, you could just output each into a file and have a simple script read each with something like `line1_file1 + " " + line1_file2` and so on. 

## Conclusion
---
These tasks were actually easier than the last competition I took part in, but I still learned a few new tricks, like `xargs -i`. Overall, pretty fun :)
